data:
  dataset_id: ruthchy/semantic-length-generalization-logo-data-desc-ascii_35
  include_ascii: true
  include_desc: false
  include_sys_prompt_fn: false
  include_sys_prompt_inf: true
logging:
  use_wandb: true
lora:
  alpha: 64
  dropout: 0
  rank: 64
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - up_proj
  - down_proj
  - o_proj
  - gate_proj
model:
  max_new_tokens: 250
  model_id: codellama/CodeLlama-7b-Instruct-hf
  temperature: 1.0
  top_k: 50
  topk_prompt: 0
  topk_train: 1
training:
  eval_steps: 100
  gradient_accumulation_steps: 1
  learning_rate: 2e-4
  logging_steps: 5
  lr_scheduler_type: cosine
  max_seq_length: 2048
  per_device_eval_batch_size: 16
  per_device_train_batch_size: 8
  prompt_loss_weight: 0.1
  random_seed: 42
  save_steps: 500
  shuffle: true
  train_epochs: 3
  warmup_ratio: 0.05
  warmup_steps: None
