{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_config(yaml_file):\n",
    "    with open(yaml_file, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config(\"config.yaml\")\n",
    "\n",
    "cuda_devices = config[\"cuda\"][\"devices\"]\n",
    "# data\n",
    "data_dir = config[\"data\"]\n",
    "# model\n",
    "model_name = config[\"model\"][\"name\"]\n",
    "#lora\n",
    "rank = int(config[\"lora\"][\"rank\"])\n",
    "alpha = int(config[\"lora\"][\"alpha\"])\n",
    "\n",
    "# training\n",
    "max_seq_length = config[\"training\"][\"max_seq_length\"]\n",
    "learning_rate = float(config[\"training\"][\"learning_rate\"])\n",
    "train_epochs = config[\"training\"][\"train_epochs\"]\n",
    "per_device_batch_size = config[\"training\"][\"per_device_batch_size\"]\n",
    "gradient_accumulation_steps = config[\"training\"][\"gradient_accumulation_steps\"]\n",
    "save_steps = config[\"training\"][\"save_steps\"]\n",
    "eval_steps = config[\"training\"][\"eval_steps\"]\n",
    "logging_steps = config[\"training\"][\"logging_steps\"]\n",
    "random_seed = config[\"training\"][\"random_seed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda devices: 2\n"
     ]
    }
   ],
   "source": [
    "### set the cuda device(s)\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cuda_devices\n",
    "print(\"cuda devices:\", cuda_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpriscillachyrva\u001b[0m (\u001b[33mpriscillachyrva-university-mannheim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ceph/pratz/GitHub_repos/master-thesis/wandb/run-20250204_163254-8mk4awdc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning/runs/8mk4awdc' target=\"_blank\">fine-tune-CodeLlama-7b-hf-semantic-length-generalization-logo-data-desc-ascii_35_202502041632</a></strong> to <a href='https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning' target=\"_blank\">https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning/runs/8mk4awdc' target=\"_blank\">https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning/runs/8mk4awdc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning/runs/8mk4awdc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb7c3fe08e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load libraries and model from HF \n",
    "import torch\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import wandb\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM # to train model only on the generated prompts\n",
    "#from trl.trainer import ConstantLengthDataset\n",
    "from datasets import load_dataset\n",
    "from _1_prompt_temp_v1 import instruction_format, conversational_format, sys_prompt\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported, apply_chat_template\n",
    "\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d%H%M\")\n",
    "# Initialize WandB (ensure you've logged in using `wandb login`)\n",
    "wandb.init(project=\"code-llama-finetuning\", \n",
    "           name=f\"fine-tune-{model_name.split('/')[-1]}-{data_dir.split('/')[-1]}_{timestamp}\",\n",
    "           config={\"learning_rate\": learning_rate, \"num_train_epochs\": train_epochs, \"max_seq_length\": max_seq_length, \"learning_rate\": learning_rate})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=rank,\n",
    "    lora_alpha=alpha,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], \n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state= random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Your task is to draw simple black and white graphics with the custom library. DO NOT USE THE BUILT-IN TURTLE LIBRARY.\\nYou will use a custom turtle library, similar to the built-in library, which is sufficient for all tasks.\\n\\nHere are all the available functions in the custom turtle library:\\n- forward(x): move forward x pixels\\n- left(theta): rotate left by theta degrees\\n- right(theta): rotate right by theta degrees\\n- penup(): stop drawing\\n- pendown(): start drawing\\n- teleport(x, y, theta): move to position (x, y) with angle theta\\n- heading(): get the current angle of the turtle\\n- isdown(): check if the pen is down\\n- embed(program, local vars): runs the code in program using the current context and teleports back to the original position. Allows you to nest programs. Implementationally, embed gets the turtle state (is down, x, y, heading), executes program, then returns to the original state.', 'role': 'system'}, {'content': 'Generate a python program producing the graphic, which is described and depicted as follows:\\n The Program draws a 5 pointed star\\n Graphic:\\n00000000000000000000000000000000000\\n00000000000000000000000120000000000\\n00000000000000000000002220000000000\\n00000000000000000000031020000000000\\n00000000000000000001300002000000000\\n00000000000000000021000002000000000\\n00000000000000000300000002000000000\\n00000000000000012000000000200000100\\n00000000000000310000000000300000200\\n00000000000013000000000000200001100\\n00000000000220000000000000020002000\\n00000000003100000000000000020002000\\n00000000130000000000000000020001000\\n00000002100000000000000000002020000\\n00000030000000000000000000002020000\\n00001200000000000000000000002020000\\n00031000000000000000000000000500000\\n00400000000000000000000000000400000\\n00031000000000000000000000000500000\\n00001200000000000000000000002020000\\n00000030000000000000000000002020000\\n00000002100000000000000000002020000\\n00000000130000000000000000020001000\\n00000000003100000000000000020002000\\n00000000000220000000000000020002000\\n00000000000013000000000000200001100\\n00000000000000310000000000300000200\\n00000122222222235222222222422222200\\n00000000000000000300000002000000000\\n00000000000000000021000002000000000\\n00000000000000000001300002000000000\\n00000000000000000000031020000000000\\n00000000000000000000002220000000000\\n00000000000000000000000120000000000\\n00000000000000000000000000000000000\\n', 'role': 'user'}, {'content': 'for i in range(5):\\n    forward(16)\\n    left(180.0 - 72.0)', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(data_dir)\n",
    "format_is_conversational = False\n",
    "\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].map(lambda x: conversational_format(x, include_description=True, include_ascii=True))\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "\n",
    "# Print out a sample to confirm the changes\n",
    "print(train_dataset[\"conversations\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Description', 'ASCII-Art', 'Program', 'conversations'],\n",
       "        num_rows: 8006\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Description', 'ASCII-Art', 'Program', 'conversations'],\n",
       "        num_rows: 997\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Description', 'ASCII-Art', 'Program', 'conversations'],\n",
       "        num_rows: 997\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    #mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2c3187fb024ed89e40aeeace2e64c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00efa7d33284fa095d9102136ebd1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7769e09c48c84e9fa72a8432e39c651d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "train_dataset, val_dataset, test_dataset = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "\n",
    "# Print out a sample to confirm the changes\n",
    "#print(train_dataset[\"conversations\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We automatically added an EOS token to stop endless generations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bc1496bb36445690620138eaee1829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036b2594c94f4ec9b420c5c12e1e0762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2889f4df9541a9a8926119c694d5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### define and apply chat template\n",
    "chat_template = \"\"\"{SYSTEM}\n",
    "### Instruction:\n",
    "{INPUT}\n",
    "### Python Program:\n",
    "{OUTPUT}\"\"\"\n",
    "\n",
    "\n",
    "dataset = apply_chat_template(\n",
    "    dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    chat_template = chat_template,\n",
    "    #default_system_message = sys_prompt, # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Description', 'ASCII-Art', 'Program', 'conversations', 'text'],\n",
       "        num_rows: 8006\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Description', 'ASCII-Art', 'Program', 'conversations', 'text'],\n",
       "        num_rows: 997\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Description', 'ASCII-Art', 'Program', 'conversations', 'text'],\n",
       "        num_rows: 997\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "Your task is to draw simple black and white graphics with the custom library. DO NOT USE THE BUILT-IN TURTLE LIBRARY.\n",
      "You will use a custom turtle library, similar to the built-in library, which is sufficient for all tasks.\n",
      "\n",
      "Here are all the available functions in the custom turtle library:\n",
      "- forward(x): move forward x pixels\n",
      "- left(theta): rotate left by theta degrees\n",
      "- right(theta): rotate right by theta degrees\n",
      "- penup(): stop drawing\n",
      "- pendown(): start drawing\n",
      "- teleport(x, y, theta): move to position (x, y) with angle theta\n",
      "- heading(): get the current angle of the turtle\n",
      "- isdown(): check if the pen is down\n",
      "- embed(program, local vars): runs the code in program using the current context and teleports back to the original position. Allows you to nest programs. Implementationally, embed gets the turtle state (is down, x, y, heading), executes program, then returns to the original state.\n",
      "<</SYS>>\n",
      "\n",
      "Generate a python program producing the graphic, which is described and depicted as follows:\n",
      " The Program draws a 5 pointed star\n",
      " Graphic:\n",
      "00000000000000000000000000000000000\n",
      "00000000000000000000000120000000000\n",
      "00000000000000000000002220000000000\n",
      "00000000000000000000031020000000000\n",
      "00000000000000000001300002000000000\n",
      "00000000000000000021000002000000000\n",
      "00000000000000000300000002000000000\n",
      "00000000000000012000000000200000100\n",
      "00000000000000310000000000300000200\n",
      "00000000000013000000000000200001100\n",
      "00000000000220000000000000020002000\n",
      "00000000003100000000000000020002000\n",
      "00000000130000000000000000020001000\n",
      "00000002100000000000000000002020000\n",
      "00000030000000000000000000002020000\n",
      "00001200000000000000000000002020000\n",
      "00031000000000000000000000000500000\n",
      "00400000000000000000000000000400000\n",
      "00031000000000000000000000000500000\n",
      "00001200000000000000000000002020000\n",
      "00000030000000000000000000002020000\n",
      "00000002100000000000000000002020000\n",
      "00000000130000000000000000020001000\n",
      "00000000003100000000000000020002000\n",
      "00000000000220000000000000020002000\n",
      "00000000000013000000000000200001100\n",
      "00000000000000310000000000300000200\n",
      "00000122222222235222222222422222200\n",
      "00000000000000000300000002000000000\n",
      "00000000000000000021000002000000000\n",
      "00000000000000000001300002000000000\n",
      "00000000000000000000031020000000000\n",
      "00000000000000000000002220000000000\n",
      "00000000000000000000000120000000000\n",
      "00000000000000000000000000000000000\n",
      " [/INST] for i in range(5):\n",
      "    forward(16)\n",
      "    left(180.0 - 72.0) </s>\n"
     ]
    }
   ],
   "source": [
    "#print(dataset[\"train\"][\"conversations\"][0])\n",
    "print(dataset[\"train\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4ad524a71a41cdb91a38a5d55ff5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/8006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b644482c8814dad89cac577f3b8e4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_1 = SFTTrainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field = \"text\", # used as model input\n",
    "    compute_metrics=None, # (Callable[[transformers.EvalPrediction], dict], optional defaults to None) â€” The function used to compute metrics during evaluation. It should return a dictionary mapping metric names to metric values. If not specified, only the loss will be computed during evaluation.\n",
    "    args = SFTConfig(\n",
    "        output_dir = f\"./results/fine-tune-{model_name.split('/')[-1]}-{data_dir.split('/')[-1]}_{timestamp}\",\n",
    "        # pre-processing\n",
    "        max_seq_length = max_seq_length,\n",
    "        packing = False,\n",
    "        dataset_num_proc =4,\n",
    "        # training parameters\n",
    "        learning_rate = learning_rate,\n",
    "        num_train_epochs = train_epochs, # or use max_steps if not training a full epoch\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        per_device_train_batch_size = per_device_batch_size,\n",
    "        per_device_eval_batch_size = per_device_batch_size,\n",
    "        # reporting and logging\n",
    "        report_to = [\"wandb\"],\n",
    "        push_to_hub = True,\n",
    "        hub_model_id = f\"fine-tune-{model_name.split('/')[-1]}-{data_dir.split('/')[-1]}_{timestamp}\",\n",
    "        logging_strategy = \"steps\",\n",
    "        logging_steps = logging_steps,\n",
    "        # checkpointing\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = save_steps,\n",
    "        # evaluation\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = eval_steps,\n",
    "        # optimization\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),    \n",
    "        # ensure reproducibility\n",
    "        seed = random_seed,\n",
    "        data_seed = random_seed,\n",
    "        full_determinism=True\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on completions only ###\n",
    "https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only\n",
    "\n",
    "orients on the instruction_format_v1 function from _1_prompt_temp_v2.py but excludes the include_description and include_ascii arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"Description\"])):\n",
    "        formated_input = f\"Generate a python program producing the graphic, which is described and depicted as follows:\\n    The Program draws {example['Description'][i]}\\n    Graphic:\\n{example['ASCII-Art'][i]}\\n\"\n",
    "        text = f\"### Instruction: {formated_input}\\n### Python Program: {example['Program'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "response_template = \"### Python Program:\" \n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "trainer_2 = SFTTrainer(\n",
    "    model,\n",
    "    data_collator=collator,          \n",
    "    formatting_func=formatting_prompts_func, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=None, # (Callable[[transformers.EvalPrediction], dict], optional defaults to None) â€” The function used to compute metrics during evaluation. It should return a dictionary mapping metric names to metric values. If not specified, only the loss will be computed during evaluation.\n",
    "    args = SFTConfig(\n",
    "        output_dir = f\"./results/fine-tune-{model_name.split('/')[-1]}-{data_dir.split('/')[-1]}_{timestamp}\",\n",
    "        # pre-processing\n",
    "        max_seq_length = max_seq_length,\n",
    "        packing = False,\n",
    "        dataset_num_proc =4,\n",
    "        # training parameters\n",
    "        learning_rate = learning_rate,\n",
    "        num_train_epochs = train_epochs,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        per_device_train_batch_size = per_device_batch_size,\n",
    "        per_device_eval_batch_size = per_device_batch_size,\n",
    "        # reporting and logging\n",
    "        report_to = [\"wandb\"],\n",
    "        push_to_hub = True,\n",
    "        hub_model_id = f\"fine-tune-{model_name.split('/')[-1]}-{data_dir.split('/')[-1]}_{timestamp}\",\n",
    "        logging_strategy = \"steps\",\n",
    "        logging_steps = logging_steps,\n",
    "        # checkpointing\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = save_steps,\n",
    "        # evaluation\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = eval_steps,\n",
    "        # optimization\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),    \n",
    "        # ensure reproducibility\n",
    "        seed = random_seed,\n",
    "        data_seed = random_seed,\n",
    "        full_determinism=True\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 8,006 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 256 | Total steps = 31\n",
      " \"-____-\"     Number of trainable parameters = 19,988,480\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 880.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 528.81 MiB is free. Process 2203578 has 27.04 GiB memory in use. Including non-PyTorch memory, this process has 19.90 GiB memory in use. Of the allocated memory 19.58 GiB is allocated by PyTorch, and 18.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     trainer_2\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrainer_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:148\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:380\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:64\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/accelerate/accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2248\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:511\u001b[0m, in \u001b[0;36mcustom_bwd.<locals>.decorate_bwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(bwd)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_bwd\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast(\n\u001b[1;32m    507\u001b[0m         device_type\u001b[38;5;241m=\u001b[39mdevice_type,\n\u001b[1;32m    508\u001b[0m         enabled\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast,\n\u001b[1;32m    509\u001b[0m         dtype\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_dtype,\n\u001b[1;32m    510\u001b[0m     ):\n\u001b[0;32m--> 511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/unsloth_zoo/gradient_checkpointing.py:170\u001b[0m, in \u001b[0;36mUnsloth_Offloaded_Gradient_Checkpointer.backward\u001b[0;34m(ctx, dY)\u001b[0m\n\u001b[1;32m    168\u001b[0m hidden_states\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 170\u001b[0m     (output,) \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(output, dY)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, hidden_states\u001b[38;5;241m.\u001b[39mgrad,) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(ctx\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/unsloth/models/llama.py:508\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    507\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm, hidden_states)\n\u001b[0;32m--> 508\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py:162\u001b[0m, in \u001b[0;36mapply_lora_mlp_swiglu\u001b[0;34m(self, X, inplace)\u001b[0m\n\u001b[1;32m    160\u001b[0m upW,     upW_quant,   upA,   upB,   upS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m  up_proj)\n\u001b[1;32m    161\u001b[0m downW, downW_quant, downA, downB, downS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj)\n\u001b[0;32m--> 162\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mLoRA_MLP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgateW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mupW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[43mupW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mupB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mupS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdownW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mswiglu_fg_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswiglu_DWf_DW_dfg_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                     \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:465\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py:78\u001b[0m, in \u001b[0;36mLoRA_MLP.forward\u001b[0;34m(ctx, X, gateW, gateW_quant, gateA, gateB, gateS, upW, upW_quant, upA, upB, upS, downW, downW_quant, downA, downB, downS, _forward_function, _backward_function, inplace)\u001b[0m\n\u001b[1;32m     76\u001b[0m g \u001b[38;5;241m=\u001b[39m matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)\n\u001b[1;32m     77\u001b[0m h \u001b[38;5;241m=\u001b[39m _forward_function(e, g)\n\u001b[0;32m---> 78\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m ctx\u001b[38;5;241m.\u001b[39mcustom_saved_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     81\u001b[0m     gateW, gateW_quant, gateS,\n\u001b[1;32m     82\u001b[0m     upW, upW_quant, upS,\n\u001b[1;32m     83\u001b[0m     downW, downW_quant, downS,\n\u001b[1;32m     84\u001b[0m     _backward_function,\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(gateA, gateB, upA, upB, downA, downB,\n\u001b[1;32m     87\u001b[0m                       X, e, g)\n",
      "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/unsloth/kernels/utils.py:412\u001b[0m, in \u001b[0;36mmatmul_lora\u001b[0;34m(X, W, W_quant, A, B, s, out)\u001b[0m\n\u001b[1;32m    409\u001b[0m     reshape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m W_quant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mdel\u001b[39;00m W\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# LoRA is enabled\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 880.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 528.81 MiB is free. Process 2203578 has 27.04 GiB memory in use. Including non-PyTorch memory, this process has 19.90 GiB memory in use. Of the allocated memory 19.58 GiB is allocated by PyTorch, and 18.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if config[\"training\"][\"trainer_output_only\"]:\n",
    "    trainer_2.train()\n",
    "else:\n",
    "    trainer_1.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReGAL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
