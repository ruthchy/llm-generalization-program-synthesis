Fine-tuning model with sample_fraction=1.0 and config_temp.yaml
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
[2025-04-16 17:43:23,593] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loaded configuration from config_temp.yaml
results/length/CodeLlama_20250416_1743
Random seed set to: 42
==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.50.3.
   \\   /|    NVIDIA RTX 6000 Ada Generation. Num GPUs = 1. Max memory: 47.383 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:22<00:22, 22.00s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:29<00:00, 13.24s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:29<00:00, 14.55s/it]
Unsloth 2025.3.18 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
codellama/CodeLlama-7b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.
Model's max position embeddings: 16384

Formatting train prompts:   0%|          | 0/8006 [00:00<?, ? examples/s]
Formatting train prompts:   4%|â–         | 347/8006 [00:00<00:02, 3350.48 examples/s]
Formatting train prompts:  10%|â–‰         | 792/8006 [00:00<00:01, 3983.02 examples/s]
Formatting train prompts:  15%|â–ˆâ–Œ        | 1224/8006 [00:00<00:01, 3685.29 examples/s]
Formatting train prompts:  21%|â–ˆâ–ˆ        | 1682/8006 [00:00<00:01, 4009.43 examples/s]
Formatting train prompts:  28%|â–ˆâ–ˆâ–Š       | 2229/8006 [00:00<00:01, 3827.54 examples/s]
Formatting train prompts:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 2683/8006 [00:00<00:01, 4033.80 examples/s]
Formatting train prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3303/8006 [00:00<00:01, 4067.41 examples/s]
Formatting train prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 3773/8006 [00:00<00:00, 4234.26 examples/s]
Formatting train prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 4223/8006 [00:01<00:00, 4086.04 examples/s]
Formatting train prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4681/8006 [00:01<00:00, 4218.10 examples/s]
Formatting train prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 5260/8006 [00:01<00:00, 4083.28 examples/s]
Formatting train prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5709/8006 [00:01<00:00, 4188.48 examples/s]
Formatting train prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 6302/8006 [00:01<00:00, 4100.37 examples/s]
Formatting train prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6750/8006 [00:01<00:00, 4194.55 examples/s]
Formatting train prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 7339/8006 [00:01<00:00, 4097.64 examples/s]
Formatting train prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 7803/8006 [00:01<00:00, 4231.66 examples/s]
Formatting train prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8006/8006 [00:01<00:00, 4033.17 examples/s]

--- DEBUG: train EXAMPLE #0 ---
System prompt included: False
FULL TEXT (with completion):
<s>[INST] Here is a gray scale image represented with integer values 0-9:
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000033000
00000000000000000000000001303000
00000000000000000000000031000200
00000000000000000000001300000300
00000000000000000000022000000300
00000000000000000000310000000020
00000000000000000013000000000031
00000000000000000210000000000033
00000000000000003000000000000014
00000000000000013000000000000004
00000000000000000310000000000033
00000000000000000013000000000031
00000000000000000000300000000020
00000000000000000000022000000300
00000000000000000122223422222422
00000000000000000000000031000200
00000000000000000000000002202000
00000000000000000000000000033000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
Please write a Python program that generates this image using our custom turtle module. [/INST] for i in range(5):
    forward(16)
    left(180.0 - 72.0) </s>
--- END DEBUG ---


Formatting validation prompts:   0%|          | 0/997 [00:00<?, ? examples/s]
Formatting validation prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 433/997 [00:00<00:00, 4165.38 examples/s]
Formatting validation prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890/997 [00:00<00:00, 4391.09 examples/s]
Formatting validation prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 997/997 [00:00<00:00, 3753.24 examples/s]

--- DEBUG: validation EXAMPLE #0 ---
System prompt included: False
FULL TEXT (with completion):
<s>[INST] Here is a gray scale image represented with integer values 0-9:
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000050000000000000
00000000000000000344200000000000
00000000000000000210120000000000
00000000000000000300030000000000
00000000000000000200120000000000
00000000000000000023200000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
00000000000000000000000000000000
Please write a Python program that generates this image using our custom turtle module. [/INST] for i in range(3):
    forward(2)
    left(120.0)
penup()
forward(2)
left(180)
pendown()
for i in range(HALF_INF):
    forward(EPS_DIST*2)
    left(EPS_ANGLE)
for i in range(HALF_INF):
    forward(EPS_DIST*2)
    left(EPS_ANGLE)
penup()
forward(20)
pendown() </s>
--- END DEBUG ---


Tokenizing train set:   0%|          | 0/8006 [00:00<?, ? examples/s]
Tokenizing train set:  12%|â–ˆâ–        | 1000/8006 [00:04<00:34, 202.13 examples/s]
Tokenizing train set:  25%|â–ˆâ–ˆâ–       | 2000/8006 [00:10<00:31, 192.71 examples/s]
Tokenizing train set:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3000/8006 [00:15<00:26, 189.45 examples/s]
Tokenizing train set:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4000/8006 [00:21<00:21, 187.88 examples/s]
Tokenizing train set:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5000/8006 [00:26<00:15, 188.22 examples/s]
Tokenizing train set:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6000/8006 [00:31<00:10, 187.21 examples/s]
Tokenizing train set:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 7000/8006 [00:37<00:05, 186.08 examples/s]
Tokenizing train set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 8000/8006 [00:42<00:00, 184.90 examples/s]
Tokenizing train set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8006/8006 [00:43<00:00, 185.45 examples/s]

Tokenizing validation set:   0%|          | 0/997 [00:00<?, ? examples/s]
Tokenizing validation set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 997/997 [00:04<00:00, 201.61 examples/s]
Tokenizing validation set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 997/997 [00:05<00:00, 193.15 examples/s]
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: priscillachyrva (priscillachyrva-university-mannheim) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.7.dev1
wandb: Run data is saved locally in /ceph/pratz/GitHub_repos/master-thesis/wandb/run-20250416_174610-q6xazl4u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run length_CodeLlama_20250416_1743
wandb: â­ï¸ View project at https://wandb.ai/priscillachyrva-university-mannheim/master-thesis
wandb: ðŸš€ View run at https://wandb.ai/priscillachyrva-university-mannheim/master-thesis/runs/q6xazl4u
Using cache found in ./models/facebookresearch_dino_main
/home/pratz/miniconda3/envs/thesis_env/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Using cache found in ./models/facebookresearch_dino_main
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 8,006 | Num Epochs = 1 | Total steps = 31
O^O/ \_/ \    Batch size per device = 32 | Gradient accumulation steps = 8
\        /    Data Parallel GPUs = 1 | Total batch size (32 x 8 x 1) = 256
 "-____-"     Trainable parameters = 1,279,262,720/7,000,000,000 (18.28% trained)
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
Using cached ./models
ðŸš€ Optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 2e-05
    maximize: False
    weight_decay: 0.001
)
ðŸš€ Scheduler: <torch.optim.lr_scheduler.LambdaLR object at 0x7fb56168c7d0>
Using cached ./models

  0%|          | 0/31 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/ceph/pratz/GitHub_repos/master-thesis/pipeline.py", line 1489, in <module>
    model = train_model(model, tokenizer, dataset, result_dir, config, timestamp, gen_type, model_type_short, wb_type)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ceph/pratz/GitHub_repos/master-thesis/pipeline.py", line 982, in train_model
    trainer.train()
  File "/home/pratz/miniconda3/envs/thesis_env/lib/python3.11/site-packages/transformers/trainer.py", line 2236, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 310, in _fast_inner_training_loop
  File "<string>", line 31, in _unsloth_training_step
  File "/ceph/pratz/GitHub_repos/master-thesis/pipeline.py", line 776, in compute_loss
    token_losses = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pratz/miniconda3/envs/thesis_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pratz/miniconda3/envs/thesis_env/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pratz/miniconda3/envs/thesis_env/lib/python3.11/site-packages/torch/nn/modules/loss.py", line 1295, in forward
    return F.cross_entropy(
           ^^^^^^^^^^^^^^^^
  File "/home/pratz/miniconda3/envs/thesis_env/lib/python3.11/site-packages/torch/nn/functional.py", line 3494, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.88 GiB. GPU 0 has a total capacity of 47.38 GiB of which 4.02 GiB is free. Including non-PyTorch memory, this process has 43.35 GiB memory in use. Of the allocated memory 42.71 GiB is allocated by PyTorch, and 133.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
An error occurred: CUDA out of memory. Tried to allocate 7.88 GiB. GPU 0 has a total capacity of 47.38 GiB of which 4.02 GiB is free. Including non-PyTorch memory, this process has 43.35 GiB memory in use. Of the allocated memory 42.71 GiB is allocated by PyTorch, and 133.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mlength_CodeLlama_20250416_1743[0m at: [34mhttps://wandb.ai/priscillachyrva-university-mannheim/master-thesis/runs/q6xazl4u[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250416_174610-q6xazl4u/logs[0m

  0%|          | 0/31 [00:19<?, ?it/s]


Trial failed with CUDA out of memory (exit code 0)