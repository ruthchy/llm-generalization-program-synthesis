{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries and model from HF\n",
    "import torch\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "from _1_prompt_templates import format_data, token_dict\n",
    "token_pos = 0\n",
    "\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "max_seq_length = 2048\n",
    "random_seed= 3407 \n",
    "\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d%H%M\")\n",
    "# Initialize WandB (ensure you've logged in using `wandb login`)\n",
    "wandb.init(project=\"code-llama-finetuning\", \n",
    "           name=f\"fine-tune-instruct-semantic-length-generalization-ascii-desc_{timestamp}\",\n",
    "           config={\"learning_rate\": 5e-5, \"num_train_epochs\": 1, \"max_seq_length\": max_seq_length, \"num_epochs\": 1,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "# if tokenizer.pad_token is None then an error will be raised therfore set it\n",
    "if tokenizer.pad_token == None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token or '[PAD]'\n",
    "    if tokenizer.pad_token == '[PAD]':\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(f\"Added padding token: {tokenizer.pad_token}\")\n",
    "\n",
    "\n",
    "# LoRA (16-bit) for PEFT => this means I need 16GB of VRAM when training the 7B-codelama model (Alternative: QLoRA could be specified with 8GB VRAM)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], \n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state= random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets and access the splits\n",
    "dataset = load_dataset(\"ruthchy/semantic-length-generalization-logo-data-desc-ascii_35\")\n",
    "print(dataset)\n",
    "train_dataset, val_dataset, test_dataset = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "\n",
    "# Tokenize the datasets\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"Input\"],  \n",
    "        text_pair=examples[\"Program\"],  \n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=True, # dynamically pad to the maximum length in the batch\n",
    "    )\n",
    "\n",
    "# Apply the tokenizer to the datasets\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, num_proc=4)\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True, num_proc=4)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Fine-Tuning Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a base TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/02_expt\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    #max_new_tokens=300, # the longest Program in test is 244 so 300 is enough\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    report_to=[\"wandb\"],\n",
    "    hub_model_id=\"ruthchy/02_expt_code-llama-ascii-desc\", \n",
    "    push_to_hub=True,\n",
    "    logging_steps = 1,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=training_args,\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model right after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(tokenized_test_dataset)\n",
    "wandb.log({\"Results\": results})\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output how long the min,avg and max token lengths are for the input and program\n",
    "def inquery_tokenlength(df):\n",
    "    # Calculate token lengths for 'Input' and 'Program' columns\n",
    "    input_lengths = [len(tokenizer.tokenize(example['Prompt'])) for example in df]\n",
    "    program_lengths = [len(tokenizer.tokenize(example['Program'])) for example in df]\n",
    "\n",
    "    # Calculate average and maximum lengths\n",
    "    avg_input_length = sum(input_lengths) / len(input_lengths)\n",
    "    min_input_length = min(input_lengths)\n",
    "    max_input_length = max(input_lengths)\n",
    "\n",
    "    avg_program_length = sum(program_lengths) / len(program_lengths)\n",
    "    min_program_length = min(program_lengths)\n",
    "    max_program_length = max(program_lengths)\n",
    "\n",
    "    return {\n",
    "        \"min_input_length\": min_input_length,\n",
    "        \"avg_input_length\": avg_input_length,\n",
    "        \"max_input_length\": max_input_length,\n",
    "        \"min_program_length\": min_program_length,\n",
    "        \"avg_program_length\": avg_program_length,\n",
    "        \"max_program_length\": max_program_length,\n",
    "    }\n",
    "\n",
    "# Print token lengths for ASCII-art + Description dataset\n",
    "print(\"ASCII-art + Description Dataset:\")\n",
    "lengths = inquery_tokenlength(test_dataset)\n",
    "for key, value in lengths.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cells are code which should decode the predictions and then load the PseudoProgramInterpreter class and the ASCIIProcessor from synthetic data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "# Convert logits to readable text\n",
    "def convert_logits_to_str(predictions):\n",
    "    predicted_token_ids = torch.tensor(predictions.predictions).argmax(dim=-1)\n",
    "    predicted_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_token_ids]\n",
    "    return predicted_texts\n",
    "\n",
    "predicted_texts = convert_logits_to_str(predictions)\n",
    "\n",
    "test_dataset_pred = tokenized_test_dataset.add_column(\"predicted_output\", predicted_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "original_sys_path = sys.path.copy()\n",
    "try:\n",
    "    synthetic_data_path = os.path.abspath(\"synthetic_data\")\n",
    "    sys.path.append(synthetic_data_path)\n",
    "    from _4_logo_graphic_generator_v1 import PseudoProgramInterpreter as PseudoProgramInterpreter_v1\n",
    "    from _5_ascii_processor import ASCIIProcessor\n",
    "finally:\n",
    "    sys.path = original_sys_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReGAL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
