{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
                        "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
                    ]
                }
            ],
            "source": [
                "import yaml\n",
                "import os\n",
                "import torch\n",
                "import pandas as pd\n",
                "import wandb\n",
                "from datasets import load_dataset\n",
                "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
                "from transformers import TextStreamer\n",
                "\n",
                "\n",
                "def load_config(yaml_file):\n",
                "    with open(yaml_file, \"r\") as file:\n",
                "        config = yaml.safe_load(file)\n",
                "    return config\n",
                "\n",
                "config = load_config(\"config.yaml\")\n",
                "\n",
                "cuda_devices = config[\"cuda\"][\"devices\"]\n",
                "# data\n",
                "data_dir = config[\"data\"]\n",
                "# model\n",
                "model_name = config[\"model\"][\"name_pc\"]\n",
                "#lora\n",
                "rank = int(config[\"lora\"][\"rank\"])\n",
                "alpha = int(config[\"lora\"][\"alpha\"])\n",
                "\n",
                "# training\n",
                "max_seq_length = config[\"training\"][\"max_seq_length\"]\n",
                "learning_rate = float(config[\"training\"][\"learning_rate\"])\n",
                "warmup_steps = int(config[\"training\"][\"warmup_steps\"])\n",
                "lr_scheduler_type = str(config[\"training\"][\"lr_scheduler_type\"])\n",
                "train_epochs = config[\"training\"][\"train_epochs\"]\n",
                "per_device_batch_size = config[\"training\"][\"per_device_batch_size\"]\n",
                "gradient_accumulation_steps = config[\"training\"][\"gradient_accumulation_steps\"]\n",
                "save_steps = config[\"training\"][\"save_steps\"]\n",
                "eval_steps = config[\"training\"][\"eval_steps\"]\n",
                "logging_steps = config[\"training\"][\"logging_steps\"]\n",
                "random_seed = config[\"training\"][\"random_seed\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "cuda devices: 1\n"
                    ]
                }
            ],
            "source": [
                "### set the cuda device(s)\n",
                "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
                "os.environ['CUDA_VISIBLE_DEVICES'] = cuda_devices\n",
                "print(\"cuda devices:\", cuda_devices)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Inference Pipeline for already fine-tuned models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpriscillachyrva\u001b[0m (\u001b[33mpriscillachyrva-university-mannheim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.19.4"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/ceph/pratz/GitHub_repos/master-thesis/wandb/run-20250208_231933-vdh51l38</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning/runs/vdh51l38' target=\"_blank\">inference-fine-tune-CodeLlama-7b-Instruct-sem-len-generalization-logo-ascii-35-completions-only-20250206-semantic-length-generalization-logo-data-desc-ascii_35_202502082319</a></strong> to <a href='https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning' target=\"_blank\">https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning/runs/vdh51l38' target=\"_blank\">https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning/runs/vdh51l38</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/priscillachyrva-university-mannheim/code-llama-finetuning/runs/vdh51l38?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
                        ],
                        "text/plain": [
                            "<wandb.sdk.wandb_run.Run at 0x7f35a56905b0>"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d%H%M\")\n",
                "# Initialize WandB (ensure you've logged in using `wandb login`)\n",
                "wandb.init(project=\"code-llama-finetuning\", \n",
                "           name=f\"inference-{model_name.split('/')[-1]}-{data_dir.split('/')[-1]}_{timestamp}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
                        "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform: Linux.\n",
                        "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
                        "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
                        " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
                        "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c9b18a08a10349ccb596dbbffd74a21b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "codellama/CodeLlama-7b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Unsloth 2024.12.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
                    ]
                }
            ],
            "source": [
                "# Model configuration\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name,\n",
                "    max_seq_length=max_seq_length,\n",
                "    load_in_4bit=True,\n",
                "    dtype=None,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load and Preprocess Train, Validation, and Test-Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "from _1_prompt_temp_v1 import conversational_format_PBE_zeroshot_inference\n",
                "from unsloth.chat_templates import apply_chat_template, get_chat_template\n",
                "tokenizer = get_chat_template(\n",
                "    tokenizer,\n",
                "    chat_template = \"llama\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
                "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
                ")\n",
                "def formatting_prompts_func(examples):\n",
                "    convos = examples[\"conversations\"]\n",
                "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
                "    return { \"text\" : texts, }\n",
                "pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[{'content': 'Here is a gray scale image represented with integer values 0-9.\\n00000000000000000000000000000000000\\n00000000000000000000000000000000000\\n00000000000000000000000000000000000\\n00000000000000000000000000000000000\\n00000000000000000000000000133320000\\n00000000000000000000000002200003000\\n00000000000000000000000021000000300\\n00000000000000000000000030000000020\\n00000000000000000000000030000000030\\n00000000000000000000000030000000030\\n00000000000000000000000030000000030\\n00000000000000000221000030000000300\\n00000000000000003000221003000002200\\n00000000000000220000002000433331000\\n00000000000000200000001435331000000\\n00000021001223420000003311002200000\\n00002301322001102000021230000300000\\n00220000033002001200030210000030000\\n00200000210202000120030300000030000\\n00200000300200300020033000000030000\\n01100000200200030020042000000020000\\n02000000200200005664652000000300000\\n02100000100200130213143200003100000\\n00300000021100300410003133330000000\\n00030000034202123000003000000000000\\n00003222252356300000001100000000000\\n00000000000003000000001100000000000\\n00000000000001200000003000000000000\\n00000000000000300000012000000000000\\n00000000000000031000230000000000000\\n00000000000000002333100000000000000\\n00000000000000000000000000000000000\\n00000000000000000000000000000000000\\n00000000000000000000000000000000000\\n00000000000000000000000000000000000\\nPlease, write a Python program that generates this image using our own custom turtle module.', 'role': 'user'}]\n"
                    ]
                }
            ],
            "source": [
                "# load the datasets and access the splits\n",
                "test_dataset = load_dataset(data_dir, split=\"test\")\n",
                "test_dataset = test_dataset.map(conversational_format_PBE_zeroshot_inference)\n",
                "#test_dataset = test_dataset.map(formatting_prompts_func, batched=True)\n",
                "print(test_dataset[\"conversations\"][0])\n",
                "#print(test_dataset[\"text\"][0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_dataset = load_dataset(data_dir, split=\"test\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "alpaca_prompt = \"\"\"[INST]### Instruction:\n",
                "{}\n",
                "[/INST]\n",
                "### Python Program:\n",
                "{}\"\"\"\n",
                "def formatting_prompts_func(examples):\n",
                "    instructions = f\"Here is a gray scale image represented with integer values 0-9.\\n{examples['ASCII-Art']}\\nPlease, write a Python program that generates this image using our own custom turtle module.\\n\"\n",
                "    programs = examples[\"programs\"]\n",
                "    texts =[]\n",
                "    for instruction, programs in zip(instructions, programs):\n",
                "        text = alpaca_prompt.format(instruction, programs)\n",
                "        texts.append(text)\n",
                "    return { \"text\" : texts, }\n",
                "pass\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prediction with pre-trained and/or fine-tuned model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = FastLanguageModel.for_inference(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<ï½œbeginâ–ofâ–sentenceï½œ>user: Here is a gray scale image represented with integer values 0-9.\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000133320000\n",
                        "00000000000000000000000002200003000\n",
                        "00000000000000000000000021000000300\n",
                        "00000000000000000000000030000000020\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000221000030000000300\n",
                        "00000000000000003000221003000002200\n",
                        "00000000000000220000002000433331000\n",
                        "00000000000000200000001435331000000\n",
                        "00000021001223420000003311002200000\n",
                        "00002301322001102000021230000300000\n",
                        "00220000033002001200030210000030000\n",
                        "00200000210202000120030300000030000\n",
                        "00200000300200300020033000000030000\n",
                        "01100000200200030020042000000020000\n",
                        "02000000200200005664652000000300000\n",
                        "02100000100200130213143200003100000\n",
                        "00300000021100300410003133330000000\n",
                        "00030000034202123000003000000000000\n",
                        "00003222252356300000001100000000000\n",
                        "00000000000003000000001100000000000\n",
                        "00000000000001200000003000000000000\n",
                        "00000000000000300000012000000000000\n",
                        "00000000000000031000230000000000000\n",
                        "00000000000000002333100000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "Please, write a Python program that generates this image using our own custom turtle module.\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conversation \u001b[38;5;129;01min\u001b[39;00m test_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversations\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m     18\u001b[0m     conversation_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m conversation])\n\u001b[0;32m---> 19\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response_streaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse for conversation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mgenerate_response_streaming\u001b[0;34m(conversation)\u001b[0m\n\u001b[1;32m      8\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Generate the response and stream the output\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# The streaming will automatically handle the output tokens\u001b[39;00m\n\u001b[1;32m     14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([streamer\u001b[38;5;241m.\u001b[39mdecode() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m streamer\u001b[38;5;241m.\u001b[39mstream()])\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/unsloth/models/llama.py:1483\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[1;32m   1481\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1483\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m \n\u001b[1;32m   1489\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/unsloth/models/llama.py:969\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    982\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/unsloth/models/llama.py:608\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# Embed positions\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 608\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# inputs_embeds = inputs_embeds.to(self.config.torch_dtype)\u001b[39;00m\n\u001b[1;32m    611\u001b[0m torch_dtype \u001b[38;5;241m=\u001b[39m __DTYPE_MAP\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtorch_dtype, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/ReGAL_env/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
                    ]
                }
            ],
            "source": [
                "#Alternative: https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-10.-train-the-model\n",
                "FastLanguageModel.for_inference(model) \n",
                "\n",
                "def generate_response_streaming(conversation):\n",
                "    inputs = tokenizer(conversation, return_tensors=\"pt\", truncation=True, padding=True)\n",
                "    \n",
                "    # Initialize the text streamer\n",
                "    streamer = TextStreamer(tokenizer)\n",
                "    \n",
                "    # Generate the response and stream the output\n",
                "    model.generate(**inputs, streamer=streamer)\n",
                "    \n",
                "    # The streaming will automatically handle the output tokens\n",
                "    response = \"\".join([streamer.decode() for token in streamer.stream()])\n",
                "    return response\n",
                "\n",
                "for conversation in test_dataset[\"conversations\"][:2]:\n",
                "    conversation_text = \"\".join([f\"{entry['role']}: {entry['content']}\\n\" for entry in conversation])\n",
                "    response = generate_response_streaming(conversation_text)\n",
                "    print(f\"Response for conversation: {response}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Alternative: https://docs.unsloth.ai/basics/inference\n",
                "FastLanguageModel.for_inference(model) \n",
                "text_streamer = TextStreamer(tokenizer)\n",
                "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 64)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "33b9c7d4185d46a6bc5eb58530049553",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<s>[INST]### Instruction: Here is a gray scale image represented with integer values 0-9.\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000133320000\n",
                        "00000000000000000000000002200003000\n",
                        "00000000000000000000000021000000300\n",
                        "00000000000000000000000030000000020\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000221000030000000300\n",
                        "00000000000000003000221003000002200\n",
                        "00000000000000220000002000433331000\n",
                        "00000000000000200000001435331000000\n",
                        "00000021001223420000003311002200000\n",
                        "00002301322001102000021230000300000\n",
                        "00220000033002001200030210000030000\n",
                        "00200000210202000120030300000030000\n",
                        "00200000300200300020033000000030000\n",
                        "01100000200200030020042000000020000\n",
                        "02000000200200005664652000000300000\n",
                        "02100000100200130213143200003100000\n",
                        "00300000021100300410003133330000000\n",
                        "00030000034202123000003000000000000\n",
                        "00003222252356300000001100000000000\n",
                        "00000000000003000000001100000000000\n",
                        "00000000000001200000003000000000000\n",
                        "00000000000000300000012000000000000\n",
                        "00000000000000031000230000000000000\n",
                        "00000000000000002333100000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "Please, write a Python program that generates this image using our own custom turtle module.\n",
                        "[/INST]</s> ï»¿using System;\n",
                        "using System.Collections.Generic;\n",
                        "using System.Linq;\n",
                        "using System.Text;\n",
                        "using System.Threading.Tasks;\n",
                        "\n",
                        "namespace _04.Fix_Emails\n",
                        "{\n",
                        "    class Program\n",
                        "    {\n",
                        "        static void Main(string[] args)\n",
                        "        {\n",
                        "            var names = Console.ReadLine().Split(' ');\n",
                        "            var emails = Console.ReadLine().Split(' ');\n",
                        "\n",
                        "            for (int i = 0; i < names.Length; i++)\n",
                        "            {\n",
                        "                if (!emails[i].EndsWith(\"us\") &&!emails[i].EndsWith(\"uk\"))\n",
                        "                {\n",
                        "                    Console.WriteLine($\"{names[i]} -> {emails[i]}\");\n",
                        "                }\n",
                        "            }\n",
                        "        }\n",
                        "    }\n",
                        "}\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Load the pre-trained model and tokenizer (fine-tuned for code generation)\n",
                "model_name = \"ruthchy/fine-tune-CodeLlama-7b-Instruct-sem-len-generalization-logo-ascii-35-completions-only-20250206\" \n",
                "#model_name = \"xu3kev/deepseekcoder-7b-logo-pbe\"\n",
                "generator = pipeline(\"text-generation\", model=model_name, device=torch.cuda.current_device())  # Use GPU if available\n",
                "\n",
                "# Input prompt to generate a Python program\n",
                "prompt = \"\"\"<s>[INST]### Instruction: Here is a gray scale image represented with integer values 0-9.\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000133320000\n",
                "00000000000000000000000002200003000\n",
                "00000000000000000000000021000000300\n",
                "00000000000000000000000030000000020\n",
                "00000000000000000000000030000000030\n",
                "00000000000000000000000030000000030\n",
                "00000000000000000000000030000000030\n",
                "00000000000000000221000030000000300\n",
                "00000000000000003000221003000002200\n",
                "00000000000000220000002000433331000\n",
                "00000000000000200000001435331000000\n",
                "00000021001223420000003311002200000\n",
                "00002301322001102000021230000300000\n",
                "00220000033002001200030210000030000\n",
                "00200000210202000120030300000030000\n",
                "00200000300200300020033000000030000\n",
                "01100000200200030020042000000020000\n",
                "02000000200200005664652000000300000\n",
                "02100000100200130213143200003100000\n",
                "00300000021100300410003133330000000\n",
                "00030000034202123000003000000000000\n",
                "00003222252356300000001100000000000\n",
                "00000000000003000000001100000000000\n",
                "00000000000001200000003000000000000\n",
                "00000000000000300000012000000000000\n",
                "00000000000000031000230000000000000\n",
                "00000000000000002333100000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "Please, write a Python program that generates this image using our own custom turtle module.\n",
                "[/INST]</s>\"\"\"\n",
                "\n",
                "# Generate the Python code\n",
                "generated_code = generator(prompt, max_length=max_seq_length, num_return_sequences=1)\n",
                "\n",
                "# Extract and print the generated Python program\n",
                "python_program = generated_code[0]['generated_text']\n",
                "print(python_program)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<s>[INST]Your task is to draw simple black and white graphics with the custom library. DO NOT USE THE BUILT-IN TURTLE LIBRARY.\n",
                        "You will use a custom turtle library, similar to the built-in library, which is sufficient for all tasks.\n",
                        "\n",
                        "Here are all the available functions in the custom turtle library:\n",
                        "- forward(x): move forward x pixels\n",
                        "- left(theta): rotate left by theta degrees\n",
                        "- right(theta): rotate right by theta degrees\n",
                        "- penup(): stop drawing\n",
                        "- pendown(): start drawing\n",
                        "- teleport(x, y, theta): move to position (x, y) with angle theta\n",
                        "- heading(): get the current angle of the turtle\n",
                        "- isdown(): check if the pen is down\n",
                        "- embed(program, local vars): runs the code in program using the current context and teleports back to the original position. Allows you to nest programs. Implementationally, embed gets the turtle state (is down, x, y, heading), executes program, then returns to the original state.\n",
                        "### Instruction: Here is a gray scale image represented with integer values 0-9.\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000133320000\n",
                        "00000000000000000000000002200003000\n",
                        "00000000000000000000000021000000300\n",
                        "00000000000000000000000030000000020\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000221000030000000300\n",
                        "00000000000000003000221003000002200\n",
                        "00000000000000220000002000433331000\n",
                        "00000000000000200000001435331000000\n",
                        "00000021001223420000003311002200000\n",
                        "00002301322001102000021230000300000\n",
                        "00220000033002001200030210000030000\n",
                        "00200000210202000120030300000030000\n",
                        "00200000300200300020033000000030000\n",
                        "01100000200200030020042000000020000\n",
                        "02000000200200005664652000000300000\n",
                        "02100000100200130213143200003100000\n",
                        "00300000021100300410003133330000000\n",
                        "00030000034202123000003000000000000\n",
                        "00003222252356300000001100000000000\n",
                        "00000000000003000000001100000000000\n",
                        "00000000000001200000003000000000000\n",
                        "00000000000000300000012000000000000\n",
                        "00000000000000031000230000000000000\n",
                        "00000000000000002333100000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "Please, write a Python program that generates this image using our own custom turtle module.\n",
                        "[/INST]</s> #include \"stdafx.h\"\n",
                        "#include \"CppUnitTest.h\"\n",
                        "\n",
                        "#include \"../Clerk/Data/Models/Transaction.h\"\n",
                        "\n",
                        "using namespace Microsoft::VisualStudio::CppUnitTestFramework;\n",
                        "\n",
                        "namespace Clerk_Tests\n",
                        "{\n",
                        "\tTEST_CLASS(TransactionTests)\n",
                        "\t{\n",
                        "\tpublic:\n",
                        "\t\tTEST_METHOD(TestTransaction)\n",
                        "\t\t{\n",
                        "\t\t\tauto transaction = std::make_shared<Transaction>();\n",
                        "\n",
                        "\t\t\ttransaction->Id = 1;\n",
                        "\t\t\ttransaction->Date = \"2020-01-01\";\n",
                        "\t\t\ttransaction->Type = \"income\";\n",
                        "\t\t\ttransaction->Amount = 100;\n",
                        "\t\t\ttransaction->Description = \"Test\";\n",
                        "\t\t\ttransaction->AccountId = 1;\n",
                        "\t\t\ttransaction->CategoryId = 1;\n",
                        "\t\t\ttransaction->Category = \"Test\";\n",
                        "\t\t\ttransaction->Account = \"Test\";\n",
                        "\t\t\ttransaction->IsTransfer = false;\n",
                        "\t\t\ttransaction->TransferAccountId = 0;\n",
                        "\t\t\ttransaction->TransferAccount = \"\";\n",
                        "\n",
                        "\t\t\tAssert::AreEqual(1, transaction->Id);\n",
                        "\t\t\tAssert::AreEqual(\"2020-01-01\", transaction->Date);\n",
                        "\t\t\tAssert::AreEqual(\"income\", transaction->Type);\n",
                        "\t\t\tAssert::AreEqual(100, transaction->Amount);\n",
                        "\t\t\tAssert::AreEqual(\"Test\", transaction->Description);\n",
                        "\t\t\tAssert::AreEqual(1, transaction->AccountId);\n",
                        "\t\t\tAssert::AreEqual(1, transaction->CategoryId);\n",
                        "\t\t\tAssert::AreEqual(\"Test\", transaction->Category);\n",
                        "\t\t\tAssert::AreEqual(\"Test\", transaction->Account);\n",
                        "\t\t\tAssert::AreEqual(false, transaction->IsTransfer);\n",
                        "\t\t\tAssert::AreEqual(0, transaction->TransferAccountId);\n",
                        "\t\t\tAssert::AreEqual(\"\", transaction->TransferAccount);\n",
                        "\t\t}\n",
                        "\t};\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Load the pre-trained model and tokenizer (fine-tuned for code generation)\n",
                "#model_name = \"ruthchy/fine-tune-CodeLlama-7b-Instruct-sem-len-generalization-logo-ascii-35-completions-only-20250206\" \n",
                "#model_name = \"xu3kev/deepseekcoder-7b-logo-pbe\"\n",
                "#generator = pipeline(\"text-generation\", model=model_name, device=torch.cuda.current_device())  # Use GPU if available\n",
                "\n",
                "# Input prompt to generate a Python program\n",
                "prompt = \"\"\"<s>[INST]Your task is to draw simple black and white graphics with the custom library. DO NOT USE THE BUILT-IN TURTLE LIBRARY.\n",
                "You will use a custom turtle library, similar to the built-in library, which is sufficient for all tasks.\n",
                "\n",
                "Here are all the available functions in the custom turtle library:\n",
                "- forward(x): move forward x pixels\n",
                "- left(theta): rotate left by theta degrees\n",
                "- right(theta): rotate right by theta degrees\n",
                "- penup(): stop drawing\n",
                "- pendown(): start drawing\n",
                "- teleport(x, y, theta): move to position (x, y) with angle theta\n",
                "- heading(): get the current angle of the turtle\n",
                "- isdown(): check if the pen is down\n",
                "- embed(program, local vars): runs the code in program using the current context and teleports back to the original position. Allows you to nest programs. Implementationally, embed gets the turtle state (is down, x, y, heading), executes program, then returns to the original state.\n",
                "### Instruction: Here is a gray scale image represented with integer values 0-9.\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000133320000\n",
                "00000000000000000000000002200003000\n",
                "00000000000000000000000021000000300\n",
                "00000000000000000000000030000000020\n",
                "00000000000000000000000030000000030\n",
                "00000000000000000000000030000000030\n",
                "00000000000000000000000030000000030\n",
                "00000000000000000221000030000000300\n",
                "00000000000000003000221003000002200\n",
                "00000000000000220000002000433331000\n",
                "00000000000000200000001435331000000\n",
                "00000021001223420000003311002200000\n",
                "00002301322001102000021230000300000\n",
                "00220000033002001200030210000030000\n",
                "00200000210202000120030300000030000\n",
                "00200000300200300020033000000030000\n",
                "01100000200200030020042000000020000\n",
                "02000000200200005664652000000300000\n",
                "02100000100200130213143200003100000\n",
                "00300000021100300410003133330000000\n",
                "00030000034202123000003000000000000\n",
                "00003222252356300000001100000000000\n",
                "00000000000003000000001100000000000\n",
                "00000000000001200000003000000000000\n",
                "00000000000000300000012000000000000\n",
                "00000000000000031000230000000000000\n",
                "00000000000000002333100000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "Please, write a Python program that generates this image using our own custom turtle module.\n",
                "[/INST]</s>\"\"\"\n",
                "\n",
                "# Generate the Python code\n",
                "generated_code = generator(prompt, max_length=max_seq_length, num_return_sequences=1)\n",
                "\n",
                "# Extract and print the generated Python program\n",
                "python_program = generated_code[0]['generated_text']\n",
                "print(python_program)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e0ed9be08c824cff96e54b350b8cb3af",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<ï½œbeginâ–ofâ–sentenceï½œ>user: Here is a gray scale image represented with integer values 0-9.\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000133320000\n",
                        "00000000000000000000000002200003000\n",
                        "00000000000000000000000021000000300\n",
                        "00000000000000000000000030000000020\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000000000030000000030\n",
                        "00000000000000000221000030000000300\n",
                        "00000000000000003000221003000002200\n",
                        "00000000000000220000002000433331000\n",
                        "00000000000000200000001435331000000\n",
                        "00000021001223420000003311002200000\n",
                        "00002301322001102000021230000300000\n",
                        "00220000033002001200030210000030000\n",
                        "00200000210202000120030300000030000\n",
                        "00200000300200300020033000000030000\n",
                        "01100000200200030020042000000020000\n",
                        "02000000200200005664652000000300000\n",
                        "02100000100200130213143200003100000\n",
                        "00300000021100300410003133330000000\n",
                        "00030000034202123000003000000000000\n",
                        "00003222252356300000001100000000000\n",
                        "00000000000003000000001100000000000\n",
                        "00000000000001200000003000000000000\n",
                        "00000000000000300000012000000000000\n",
                        "00000000000000031000230000000000000\n",
                        "00000000000000002333100000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "00000000000000000000000000000000000\n",
                        "Please, write a Python program that generates this image using our own custom turtle module.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# Load the pre-trained model and tokenizer (fine-tuned for code generation)\n",
                "#model_name = \"ruthchy/fine-tune-CodeLlama-7b-Instruct-sem-len-generalization-logo-ascii-35-completions-only-20250206\" \n",
                "model_name = \"xu3kev/deepseekcoder-7b-logo-pbe\"\n",
                "generator = pipeline(\"text-generation\", model=model_name, device=0)  # Use GPU if available\n",
                "\n",
                "# Input prompt to generate a Python program\n",
                "prompt = \"\"\"<ï½œbeginâ–ofâ–sentenceï½œ>user: Here is a gray scale image represented with integer values 0-9.\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000133320000\n",
                "00000000000000000000000002200003000\n",
                "00000000000000000000000021000000300\n",
                "00000000000000000000000030000000020\n",
                "00000000000000000000000030000000030\n",
                "00000000000000000000000030000000030\n",
                "00000000000000000000000030000000030\n",
                "00000000000000000221000030000000300\n",
                "00000000000000003000221003000002200\n",
                "00000000000000220000002000433331000\n",
                "00000000000000200000001435331000000\n",
                "00000021001223420000003311002200000\n",
                "00002301322001102000021230000300000\n",
                "00220000033002001200030210000030000\n",
                "00200000210202000120030300000030000\n",
                "00200000300200300020033000000030000\n",
                "01100000200200030020042000000020000\n",
                "02000000200200005664652000000300000\n",
                "02100000100200130213143200003100000\n",
                "00300000021100300410003133330000000\n",
                "00030000034202123000003000000000000\n",
                "00003222252356300000001100000000000\n",
                "00000000000003000000001100000000000\n",
                "00000000000001200000003000000000000\n",
                "00000000000000300000012000000000000\n",
                "00000000000000031000230000000000000\n",
                "00000000000000002333100000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "00000000000000000000000000000000000\n",
                "Please, write a Python program that generates this image using our own custom turtle module.\"\"\"\n",
                "\n",
                "# Generate the Python code\n",
                "generated_code = generator(prompt, max_length=max_seq_length, num_return_sequences=1)\n",
                "\n",
                "# Extract and print the generated Python program\n",
                "python_program = generated_code[0]['generated_text']\n",
                "print(python_program)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ReGAL_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
