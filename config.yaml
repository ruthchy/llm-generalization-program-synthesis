cuda:
  devices: '0' # or comma separated list of device ids e.g. '0,3'

logging:
  use_wandb: False

data: 
  dataset_id: "ruthchy/semantic-length-generalization-logo-data-desc-ascii_35"
  include_desc: False
  include_ascii: True
  include_sys_prompt: False

model:
  model_id: "codellama/CodeLlama-7b-Instruct-hf"
  #model_id: "deepseek-ai/deepseek-coder-6.7b-base" 
  # already finetuned on logo data => inference only models
  #model_id: "xu3kev/deepseekcoder-7b-logo-pbe" 
  #model_id: "ruthchy/fine-tune-CodeLlama-7b-Instruct-sem-len-generalization-logo-ascii-35-completions-only-20250206"
  topk_train: 1 # no. of I/O pairs during finetuning
  topk_prompt: 0 # no. of I/O pairs during inference in addition to the I (zero-shot prompting = 0, â€¦)

training:
  prompt_loss_weight: 0.1
  completion_loss_weight: 1.0
  # pre-processing
  max_seq_length: 2048
  # training
  learning_rate: 2e-4 # 2e-4, 1e-4, 5e-5, 2e-5 recommended 2e-04=2e-4 in PBE paper (smaller lr slows learning down potentially higher accuracy)
  warmup_steps: 20 # 20, 100, 500, 1000 recommended 20 in PBE paper
  lr_scheduler_type: "cosine" # "linear" (default), "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup" # linear and cosine are usually; used cosine is used in PBE
  train_epochs: 1
  per_device_batch_size: 32 # 4, 8, 16, 32, 64, 128, 256, 512 recommended 64 in PBE 
  gradient_accumulation_steps: 8 # 4
  # checkpointing and evaluation
  save_steps: 10
  eval_steps: 1
  logging_steps: 1
  # reproducibility
  random_seed: 42
  shuffle: True # whether to shuffle the training data during training

lora:
  rank: 8 # 8, 16, 32, 64, 128 (recommended) 512 in PBE paper 
  alpha: 32 #512
  dropout: 0 #0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"]

# the higher the rank and alpha, the more expressive the model can cause offerfitting